import os
import torch
import logging
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
from huggingface_hub import login
from peft import LoraConfig, get_peft_model

# ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
MODEL_ID = "meta-llama/Llama-2-7b-chat-hf"
OUTPUT_DIR = "./llama2-finetuned"
HF_REPO = "username/llama2-finetuned"  # –ù–ê–î–û –ü–û–ú–ï–ù–Ø–¢–¨ –ù–ê –°–í–û–Æ –†–ï–ü–£!

# ‚úÖ –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ Hugging Face
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

if not HF_TOKEN:
    logging.error("‚ùå HUGGINGFACE_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –£–∫–∞–∂–∏—Ç–µ —Ç–æ–∫–µ–Ω –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö GitHub.")
    exit(1)

# ‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Hugging Face
login(HF_TOKEN)

# ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∑–∞–º–µ–Ω–∏ `dataset_name` –Ω–∞ —Å–≤–æ–π –¥–∞—Ç–∞—Å–µ—Ç)
dataset_name = "timdettmers/openassistant-guanaco"  # –ò–ª–∏ –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Ñ–∞–π–ª—É .json/.csv
dataset = load_dataset(dataset_name, split="train")

# ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å LoRA (–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ)
logging.info("‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å 4-–±–∏—Ç–Ω—ã–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º...")

from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    token=HF_TOKEN
)

# ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
peft_config = LoraConfig(
    r=8, 
    lora_alpha=16, 
    target_modules=["q_proj", "v_proj"],  # –î–æ–æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    logging_dir="./logs",
    logging_steps=100,
    report_to="none",
    push_to_hub=True,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ Hugging Face
    hub_model_id=HF_REPO,
    hub_token=HF_TOKEN
)

# ‚úÖ –°–æ–∑–¥–∞–µ–º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    tokenizer=tokenizer
)

# ‚úÖ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
logging.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
trainer.train()

# ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ Hugging Face Hub
logging.info(f"üì§ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ Hugging Face: {HF_REPO}")
trainer.push_to_hub()
