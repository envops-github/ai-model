import os
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import logging
import time
import traceback
from huggingface_hub import login

# ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logging.info("üöÄ Starting Llama 2 AI application...")

# ‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
MODEL_ID = "meta-llama/Llama-2-7b-chat-hf"

# ‚úÖ –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ Hugging Face –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

if not HF_TOKEN:
    logging.error("‚ùå HUGGINGFACE_TOKEN is not set! Please provide a valid Hugging Face API token.")
    exit(1)

# ‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Hugging Face
logging.info("üîë Logging into Hugging Face...")
login(HF_TOKEN)

# ‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU
os.environ["CUDA_VISIBLE_DEVICES"] = ""
device = torch.device("cpu")

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
try:
    logging.info("‚è≥ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_auth_token=HF_TOKEN)
except Exception as e:
    logging.error(f"üî• Failed to load tokenizer: {e}")
    exit(1)

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º
try:
    logging.info("‚è≥ Loading model with 4-bit quantization on CPU...")

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=quantization_config,
        device_map={"": device},
        use_auth_token=HF_TOKEN  # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–æ–∫–µ–Ω –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –∑–∞–∫—Ä—ã—Ç–æ–π –º–æ–¥–µ–ª–∏
    )

    logging.info(f"‚úÖ Successfully loaded model on {device}: {MODEL_ID}")
except Exception as e:
    logging.error(f"üî• Failed to load model: {e}")
    exit(1)

# ‚úÖ –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
def generate_text(prompt):
    logging.info(f"üìù Received prompt: {prompt}")

    try:
        system_instruction = "You are an AI assistant. Answer questions concisely and informatively.\n"
        full_prompt = system_instruction + prompt

        inputs = tokenizer(full_prompt, return_tensors="pt", truncation=True, padding=True).to(device)
        
        outputs = model.generate(
            **inputs,
            max_length=200,
            temperature=0.7,
            repetition_penalty=1.2,
            top_k=50,
            top_p=0.9
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logging.info(f"ü§ñ AI Response: {response}")
        return response
    except Exception as e:
        logging.error(f"‚ùå Error in text generation: {e}")
        return "Error processing your request."

# ‚úÖ Gradio UI
ui = gr.Interface(
    fn=generate_text,
    inputs=gr.Textbox(placeholder="Type your message...", lines=2, label="üí¨ Enter your prompt"),
    outputs=gr.Textbox(label="ü§ñ Llama 2 Response"),
    title="üåå Llama 2 AI Chatbot",
    description="üöÄ A chatbot powered by Llama 2 (7B) with 4-bit quantization.",
)

# ‚úÖ –ó–∞–ø—É—Å–∫ Gradio UI –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
try:
    if __name__ == "__main__":
        logging.info("üöÄ Starting UI on port 7860...")
        ui.launch(server_name="0.0.0.0", server_port=7860)

        while True:
            time.sleep(60)
except Exception as e:
    logging.error(f"‚ùå Unhandled exception: {e}")
    logging.error(traceback.format_exc())
    exit(1)
