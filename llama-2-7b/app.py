import os
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import logging
import traceback
from huggingface_hub import login

# ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logging.info("üöÄ Starting Llama 2 AI application...")

# ‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
MODEL_ID = "meta-llama/Llama-2-7b-chat-hf"

# ‚úÖ –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ Hugging Face –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

if not HF_TOKEN:
    logging.error("‚ùå HUGGINGFACE_TOKEN is not set! Please provide a valid Hugging Face API token.")
    exit(1)

# ‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Hugging Face
logging.info("üîë Logging into Hugging Face...")
login(HF_TOKEN)

# ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
if torch.cuda.is_available():
    device = "cuda"
    logging.info("‚úÖ Using CUDA for inference!")
else:
    logging.error("‚ùå No CUDA available! Please check GPU drivers.")
    exit(1)

# ‚úÖ –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º VRAM –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
torch.cuda.empty_cache()
torch.cuda.ipc_collect()

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
try:
    logging.info("‚è≥ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)
    
    # ‚úÖ –§–∏–∫—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–≥–æ PAD-—Ç–æ–∫–µ–Ω–∞
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logging.info("‚úÖ Assigned eos_token as pad_token to tokenizer.")
except Exception as e:
    logging.error(f"üî• Failed to load tokenizer: {e}")
    exit(1)

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 4-–±–∏—Ç–Ω—ã–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º
try:
    logging.info("‚è≥ Loading model with 4-bit quantization on GPU...")

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,  # ‚ö° 4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (—ç–∫–æ–Ω–æ–º–∏—è VRAM)
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.float16,  # ‚ö° –≠–∫–æ–Ω–æ–º–∏—è VRAM
        low_cpu_mem_usage=True,  # ‚ö° –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        token=HF_TOKEN
    )

    logging.info(f"‚úÖ Successfully loaded model on {device}: {MODEL_ID}")
except Exception as e:
    logging.error(f"üî• Failed to load model: {e}")
    exit(1)

# ‚úÖ –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
def generate_text(prompt):
    logging.info(f"üìù Received prompt: {prompt}")

    try:
        system_instruction = "You are an AI assistant. Answer questions concisely and informatively.\n"
        full_prompt = system_instruction + prompt

        inputs = tokenizer(
            full_prompt, 
            return_tensors="pt", 
            truncation=True, 
            padding=True,  # ‚úÖ –¢–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç!
            max_length=512  # ‚úÖ –§–∏–∫—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
        ).to(device)

        outputs = model.generate(
            **inputs,
            max_length=200,
            temperature=0.7,
            repetition_penalty=1.2,
            top_k=50,
            top_p=0.9
        )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logging.info(f"ü§ñ AI Response: {response}")
        return response
    except Exception as e:
        logging.error(f"‚ùå Error in text generation: {e}")
        return "Error processing your request."

# ‚úÖ Gradio UI
ui = gr.Interface(
    fn=generate_text,
    inputs=gr.Textbox(placeholder="Type your message...", lines=2, label="üí¨ Enter your prompt"),
    outputs=gr.Textbox(label="ü§ñ Llama 2 Response"),
    title="üåå EnvOps AI Chatbot ",
    description="üöÄ A chatbot powered by Llama 2 (7B) with 4-bit quantization on GPU by EnvOps.",
)

# ‚úÖ –ó–∞–ø—É—Å–∫ Gradio UI
try:
    if __name__ == "__main__":
        logging.info("üöÄ Starting UI on port 7860...")
        ui.launch(server_name="0.0.0.0", server_port=7860)
except Exception as e:
    logging.error(f"‚ùå Unhandled exception: {e}")
    logging.error(traceback.format_exc())
    exit(1)
